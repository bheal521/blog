<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ben Healy</title><link href="http://bheal521.github.io/" rel="alternate"></link><link href="http://bheal521.github.io/feeds/all.atom.xml" rel="self"></link><id>http://bheal521.github.io/</id><updated>2016-11-29T00:00:00-05:00</updated><entry><title>Boston Property Values</title><link href="http://bheal521.github.io/posts/2016/Boston-Property-Values/" rel="alternate"></link><published>2016-11-29T00:00:00-05:00</published><updated>2016-11-29T00:00:00-05:00</updated><author><name>Ben Healy</name></author><id>tag:bheal521.github.io,2016-11-29:/posts/2016/Boston-Property-Values/</id><summary type="html">&lt;p&gt;Using Boston open city data to visualize the property values of Boston residences&lt;/p&gt;</summary><content type="html">&lt;p&gt;Boston has recently followed a number of other cities' example in making a number of their data sources available to the public. 
Data on topics spanning services that the city offers its residents (trash pickup, transportation, etc) to permitting and health is 
made available on &lt;a href="https://data.cityofboston.gov/"&gt;Data Boston&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Mrs. and I have been torturing ourselves lately going to open houses around Boston. 
Most properties we are actually interested in are preposterously out of our price range but realtors have to take you seriously so each Saturday I 
find myself touring some multi-million dollar condo, snacking on the free food and asking what I'm sure are dumb questions about condo fees, parking availability, 
and portion of the building that is owner-occupied. &lt;/p&gt;
&lt;p&gt;This self imposed torture had me wondering about publically available housing valuations. 
So I went and found the &lt;a href="https://data.cityofboston.gov/Permitting/Property-Assessment-2016/i7w8-ure5"&gt;property assessment data&lt;/a&gt; for 2016 to take a 
look at the difference in residential property values across the city of Boston.&lt;/p&gt;
&lt;p&gt;Load a few of the packages that we will need to visualize our results and read in the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;ggmap&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;RColorBrewer&lt;span class="p"&gt;)&lt;/span&gt;
properties &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;C:/Users/Ben/Documents/GitHub/Boston Property/Property_Assessment_2016.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kp"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to compare the relative values of property in different areas of Boston I want to create a dollars per sq. foot variable. But as is expected there are lots of properties in here that don't have square footage information. Properties in this data also aren't just residential, so I needed to figure out how to discern what was commercial in order to remove it.&lt;/p&gt;
&lt;p&gt;Along with the data set there was a &lt;a href="https://data.cityofboston.gov/api/views/i7w8-ure5/files/b761b235-a98b-42cb-a7af-1f656b5c59f8?download=true&amp;amp;filename=Assessing%20Property%20Data%20Key.pdf"&gt;data dictionary&lt;/a&gt; provided. The &lt;em&gt;LU&lt;/em&gt; ("land use") variable designated the type of property a given record is so I used that to weed out the property types that were oviously not residential.&lt;/p&gt;
&lt;p&gt;I started with nearly 170,000 records, how many have missing square footage data and what are some summary stats about the distribution of non missing suqare footage data?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;First I need to get rid of the ~3,500 properties without sq footage info but there are also clearly some outliers in here. The largest property is listed as being 1.94 million square feet. Definitely don't want to include that. After we clean up those things what does the distribution of square footage look like?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;properties &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; properties&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="kp"&gt;is.na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;#get rid of all non residential properties&lt;/span&gt;
properties &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; properties&lt;span class="p"&gt;[&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LU &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;R1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;R2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;R3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;R4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;A&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;CD&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;CC&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;RC&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; 

&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kp"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that records with mising square footage information along with some of the obvious commercial properies have been removed we can see that the first quartile of square footage runs from 0 to 900 square feet and the third quartile runs from ~2,000 square feet to almost 2,500 square feet. That feels about right, though there are still some oddities. We don't want any properties with a listed size of 0 square feet nor do we want something that is 541,000 square feet. What property could that be?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;properties&lt;span class="p"&gt;[&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="m"&gt;500000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ST_NUM&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ST_NAME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ST_NAME_SUF&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;UNIT_NUM&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ZIPCODE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;LU&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A quick &lt;a href="https://www.google.com/search?q=121+nashua+street+boston+ma&amp;amp;oq=121+nashua+street+&amp;amp;aqs=chrome.0.0l2j69i57.2513j0j7&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8"&gt;google search&lt;/a&gt; of what that property is reveals that we're looking at the parking deck for the TD Garden... The weird thing is that it is listed as a land use of &lt;em&gt;A&lt;/em&gt; which stands for: "Residential 7 or more units". If we look at the histogram of square footage we find the follwing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hist&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA&lt;span class="p"&gt;,&lt;/span&gt; breaks&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="kp"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;%/%&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="m"&gt;+1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
hist&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="p"&gt;[&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="m"&gt;20000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA&lt;span class="p"&gt;,&lt;/span&gt; breaks&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;20000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/blog/master/content/images/properties_histogram1.PNG" alt="properties-histogram1" width="100%", height="100%"&gt;
&lt;img src="https://raw.githubusercontent.com/bheal521/blog/master/content/images/properties_histogram2.PNG" alt="properties-histogram2" width="100%", height="100%"&gt;&lt;/p&gt;
&lt;p&gt;As would be expected the bulk of properties we have left in the data are between 100 and 5,000 square feet. For simplicity I will trim the data to only include things larger than 250 square feet (assumed to be a data error) and things less than 5,000 square feet. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kp"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="p"&gt;)&lt;/span&gt;
properties &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; properties&lt;span class="p"&gt;[&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="m"&gt;250&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; properties&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="m"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="kp"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After cleaning out those oddly sized properties I only lost about ~5,000 records and now have a total of 124,000 (down from my original set of nearly 170k). In order to visualize these on a map I need the latitude and longitude values in the data to be accurate as well. Are there any with missing latitude or longitude values? The data read these values in as factors so first I need to convert them to numeric.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LONGITUDE2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;as.numeric&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LONGITUDE&lt;span class="p"&gt;))&lt;/span&gt;
properties&lt;span class="o"&gt;$&lt;/span&gt;LATITUDE2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;as.numeric&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LATITUDE&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LONGITUDE2&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LATITUDE2&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Converting these values to numeric turned almost half of the records values into NA's. Look at a few examples of where this happened to see if I did something wrong.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kp"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;is.na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LONGITUDE2&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;LATITUDE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;LONGITUDE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;LATITUDE2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;LONGITUDE2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Bummer, looks like a ton of the properties have missing lat/lon values. Double check that this is actually what happened throughout the other records.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kp"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="p"&gt;[&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LATITUDE&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#N/A&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="kp"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="p"&gt;[&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LONGITUDE&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#N/A&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Confirmed that this is what happened. Maybe I can generate the lat/lon values on these properties using their street addresses. Try the first record with and #NA in the lat and lon values. Does it have a valid street address?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kp"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="p"&gt;[&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LATITUDE &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;#N/A&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ST_NUM&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ST_NAME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ST_NAME_SUF&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;UNIT_NUM&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ZIPCODE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There is a limit on the Google geocoding API of 2,500 per day. But it actually look like a lot of this data is the same street address and the various units within a particular building. To maximize the amount of geocoding we can do in a single day I can create a dataframe with just the unique street number and names to batch geocode them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;geocode.properties &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;is.na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LATITUDE2&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="kp"&gt;is.na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LONGITUDE&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ST_NUM&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ST_NAME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ST_NAME_SUF&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ZIPCODE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="kp"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;geocode.properties&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are only about 12,600 unique street addresses for the ~60k properties with missing lat/lon data. It will still take 5+ days to get all of these addresses geocoded, but that's better than the month it would have taken otherwise! For today I will geocode the top 2,500 properties based on the number of units they have. Basically, if I can get a buildings lat/lon that has 50 units I'd rather do that than a building with 2 units. At least to start.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;sqldf&lt;span class="p"&gt;)&lt;/span&gt;
geocode.properties &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; sqldf&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;select ST_NUM, ST_NAME, ST_NAME_SUF, ZIPCODE, count(*) as Freq&lt;/span&gt;
&lt;span class="s"&gt;                            from properties where LATITUDE = &amp;#39;#N/A&amp;#39; group by ST_NUM, ST_NAME, ST_NAME_SUF, ZIPCODE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
geocode.properties &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; geocode.properties&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;geocode.properties&lt;span class="o"&gt;$&lt;/span&gt;Freq&lt;span class="p"&gt;,&lt;/span&gt; decreasing &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
geocode.properties&lt;span class="o"&gt;$&lt;/span&gt;number &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="kp"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;geocode.properties&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that the properties are in the right order, go through the top 2,500 and have them geocoded.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;geocode.properties&lt;span class="o"&gt;$&lt;/span&gt;Lat &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kc"&gt;NA&lt;/span&gt;
geocode.properties&lt;span class="o"&gt;$&lt;/span&gt;Lon &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kc"&gt;NA&lt;/span&gt;

&lt;span class="kr"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;i &lt;span class="kr"&gt;in&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)){&lt;/span&gt;
  temp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; geocode&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;geocode.properties&lt;span class="p"&gt;[&lt;/span&gt;geocode.properties&lt;span class="o"&gt;$&lt;/span&gt;number &lt;span class="o"&gt;==&lt;/span&gt;i&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ST_NUM&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]),&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;geocode.properties&lt;span class="p"&gt;[&lt;/span&gt;geocode.properties&lt;span class="o"&gt;$&lt;/span&gt;number &lt;span class="o"&gt;==&lt;/span&gt;i&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ST_NAME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]),&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;geocode.properties&lt;span class="p"&gt;[&lt;/span&gt;geocode.properties&lt;span class="o"&gt;$&lt;/span&gt;number &lt;span class="o"&gt;==&lt;/span&gt;i&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ST_NAME_SUF&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]),&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;, Boston, MA&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; messaging &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;FALSE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  geocode.properties&lt;span class="p"&gt;[&lt;/span&gt;geocode.properties&lt;span class="o"&gt;$&lt;/span&gt;number&lt;span class="o"&gt;==&lt;/span&gt;i&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; temp&lt;span class="o"&gt;$&lt;/span&gt;lat
  geocode.properties&lt;span class="p"&gt;[&lt;/span&gt;geocode.properties&lt;span class="o"&gt;$&lt;/span&gt;number&lt;span class="o"&gt;==&lt;/span&gt;i&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Lon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; temp&lt;span class="o"&gt;$&lt;/span&gt;lon
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that We have at least some of the missing lat/lon values that should be in this data -- merge it back to the original data source. First split out the original data into that with lat/lon data and that without. Add the new lat/lon data that we have then concatenate the two dataframes back together. Finally we need to create the dollars per square footage metric so that we can compare properties.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with.geo &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; properties&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LATITUDE &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;#N/A&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; properties&lt;span class="o"&gt;$&lt;/span&gt;LONGITUDE &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;#N/A&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
without.geo &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; properties&lt;span class="p"&gt;[(&lt;/span&gt;properties&lt;span class="o"&gt;$&lt;/span&gt;LATITUDE &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;#N/A&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; properties&lt;span class="o"&gt;$&lt;/span&gt;LONGITUDE &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;#N/A&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;

without.geo &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; sqldf&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;select a.*, b.Lat, b.Lon from `without.geo`as a left join `geocode.properties` as b on a.ST_NUM= b.ST_NUM and a.ST_NAME=b.ST_NAME and a.ST_NAME_SUF= b.ST_NAME_SUF and a.ZIPCODE=b.ZIPCODE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

without.geo&lt;span class="o"&gt;$&lt;/span&gt;Lat &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;as.numeric&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;without.geo&lt;span class="o"&gt;$&lt;/span&gt;Lat&lt;span class="p"&gt;))&lt;/span&gt;
without.geo&lt;span class="o"&gt;$&lt;/span&gt;Lon &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;as.numeric&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;without.geo&lt;span class="o"&gt;$&lt;/span&gt;Lon&lt;span class="p"&gt;))&lt;/span&gt;

without.geo&lt;span class="o"&gt;$&lt;/span&gt;LATITUDE2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; without.geo&lt;span class="o"&gt;$&lt;/span&gt;Lat
without.geo&lt;span class="o"&gt;$&lt;/span&gt;LONGITUDE2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; without.geo&lt;span class="o"&gt;$&lt;/span&gt;Lon

without.geo&lt;span class="o"&gt;$&lt;/span&gt;Lat &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kc"&gt;NULL&lt;/span&gt;
without.geo&lt;span class="o"&gt;$&lt;/span&gt;Lon &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kc"&gt;NULL&lt;/span&gt;

properties.final &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;rbind.data.frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;with.geo&lt;span class="p"&gt;,&lt;/span&gt; without.geo&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally take a look at the dollar values that are provided for the properties in this dataset.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties.final&lt;span class="o"&gt;$&lt;/span&gt;AV_TOTAL&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are only a few outrageously valued properties and after googling some it looks like a handful are actual penthouses while others are entire building values. To make things quick and easy I'm going to remove any properties that have a value of more than $7.5M and anything less than $100k.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;properties.final &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; properties.final&lt;span class="p"&gt;[&lt;/span&gt;properties.final&lt;span class="o"&gt;$&lt;/span&gt;AV_TOTAL&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="m"&gt;100000&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; properties.final&lt;span class="o"&gt;$&lt;/span&gt;AV_TOTAL&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="m"&gt;7500000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
properties.final&lt;span class="o"&gt;$&lt;/span&gt;Dollars_per_SqFt &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; properties.final&lt;span class="o"&gt;$&lt;/span&gt;AV_TOTAL&lt;span class="o"&gt;/&lt;/span&gt;properties.final&lt;span class="o"&gt;$&lt;/span&gt;LIVING_AREA
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are still some properties with wild dollars per sq. foot valuations. I'm talking more than $50k per sq foot. Quickly going to cut out properties that are valued more than $1,000 per sq foot.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;properties.final &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; properties.final&lt;span class="p"&gt;[&lt;/span&gt;properties.final&lt;span class="o"&gt;$&lt;/span&gt;Dollars_per_SqFt&lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="m"&gt;1000&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; properties.final&lt;span class="o"&gt;$&lt;/span&gt;Dollars_per_SqFt&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="kp"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;properties.final&lt;span class="p"&gt;)&lt;/span&gt;
hist&lt;span class="p"&gt;(&lt;/span&gt;properties.final&lt;span class="o"&gt;$&lt;/span&gt;Dollars_per_SqFt&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/blog/master/content/images/properties_histogram3.PNG" alt="properties-histogram2" width="100%", height="100%"&gt;&lt;/p&gt;
&lt;p&gt;Now that we're done prepping the data set, take a look at these properties over a map of Boston.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;base.map &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; ggmap&lt;span class="p"&gt;(&lt;/span&gt;get_map&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Boston, Massachusetts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                       zoom&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                       &lt;span class="kn"&gt;source&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;google&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                       maptype&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;terrain&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
boston.prop.box &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; make_bbox&lt;span class="p"&gt;(&lt;/span&gt;properties.final&lt;span class="o"&gt;$&lt;/span&gt;LONGITUDE2&lt;span class="p"&gt;,&lt;/span&gt; properties.final&lt;span class="o"&gt;$&lt;/span&gt;LATITUDE2&lt;span class="p"&gt;,&lt;/span&gt; f &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
map &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; ggmap&lt;span class="p"&gt;(&lt;/span&gt;get_map&lt;span class="p"&gt;(&lt;/span&gt;boston.prop.box&lt;span class="p"&gt;))&lt;/span&gt;

map &lt;span class="o"&gt;+&lt;/span&gt;  
  geom_jitter&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;=&lt;/span&gt; properties.final &lt;span class="p"&gt;,&lt;/span&gt;aes&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="o"&gt;=&lt;/span&gt;LONGITUDE2&lt;span class="p"&gt;,&lt;/span&gt; y&lt;span class="o"&gt;=&lt;/span&gt;LATITUDE2&lt;span class="p"&gt;,&lt;/span&gt;
             color &lt;span class="o"&gt;=&lt;/span&gt; Dollars_per_SqFt&lt;span class="p"&gt;),&lt;/span&gt; alpha&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;.15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;size&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
  scale_color_gradientn&lt;span class="p"&gt;(&lt;/span&gt;colours&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kp"&gt;rev&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;brewer.pal&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Spectral&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/blog/master/content/images/Boston Property Prices.png" alt="Boston-Property-Prices-Map" width="100%", height="100%"&gt;&lt;/p&gt;
&lt;p&gt;No surprises here, the South End and Back Bay are crazy expensive. Plenty of properties that are close to $1,000 per square foot. The North End also has some high prices, likely due to the tiny apartments that still have pretty significant price tags. What I think is more interesting is the string of more expensive places that seem to follow the Orange line out towards Jamaica Plain. I'm also surprised at how expensive the Allston properties are. &lt;/p&gt;
&lt;p&gt;Dorchester and East Boston are the neighborhoods where you can get the most for your money but what is clear is that while the value of property has been on the rise in many Boston neighborhoods and surrounding areas (Cambridge and Somerville) -- these neighborhoods are somehow not seeing those benefits.&lt;/p&gt;
&lt;p&gt;If you're interested in the code itself, I've pushed the R notebook to my github here: &lt;a href="https://github.com/bheal521/Boston-Property-Open-Data"&gt;Boston Properties R Notebook&lt;/a&gt;&lt;/p&gt;</content><category term="Maps"></category><category term="Open Data"></category></entry><entry><title>Using Travis CI</title><link href="http://bheal521.github.io/posts/2016/Automate-with-Travis-CI/" rel="alternate"></link><published>2016-04-06T00:00:00-04:00</published><updated>2016-04-06T00:00:00-04:00</updated><author><name>Ben Healy</name></author><id>tag:bheal521.github.io,2016-04-06:/posts/2016/Automate-with-Travis-CI/</id><summary type="html">&lt;p&gt;Simplifying the blog one step further with the help of a new tool.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Static site generators like Pelican are great for their simplicity. But my current workflow still requires several steps:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1. Write the blog post in Markdown
2. Run the python script to turn the Markdown files into HTML
3. Use GitHub to upload the newest version of both the source blog files (written in markdown) and the published website files.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Travis helps eliminate the later steps by watching the GitHub repo with the blog source files, and when there is a noticeable change -- it automatically runs the python script to turn the Markdown to HTML and push the new files to the published repo.&lt;/p&gt;
&lt;p&gt;Thanks to the helpful instructions from sites like Kevin Yap's it was pretty straighforward to set up too: &lt;a href="http://kevinyap.ca/2014/06/deploying-pelican-sites-using-travis-ci/"&gt;http://kevinyap.ca/2014/06/deploying-pelican-sites-using-travis-ci/&lt;/a&gt;.&lt;/p&gt;</content><category term="Blog"></category><category term="Automate"></category></entry><entry><title>Hometown Heros</title><link href="http://bheal521.github.io/posts/2016/Hometown-Heros/" rel="alternate"></link><published>2016-04-04T00:00:00-04:00</published><updated>2016-04-04T00:00:00-04:00</updated><author><name>Ben Healy</name></author><id>tag:bheal521.github.io,2016-04-04:/posts/2016/Hometown-Heros/</id><summary type="html">&lt;p&gt;Where are college bball players from?&lt;/p&gt;</summary><content type="html">&lt;p&gt;After watching too many hours of college basketball the past few weeks I started
wondering where all these kids were from. And they really are kids -- it's shocking how
young some of them are even though I look like I could be some of their children.&lt;/p&gt;
&lt;p&gt;Being somewhat familiar with the recent recruiting strategy for the UMass basketball team,
I started wondering if all colleges had focus areas for recruiting. Or were most successful
schools recruiting in their own back yard? Success being the distinction -- UMass hasn't seen much
of that recently. Or maybe there's a region of the country where all the top talent is coming from?
Are all of the top schools fighting over the talent being produced in one area?&lt;/p&gt;
&lt;p&gt;For each of the 68 (64 + play in teams...) that made this year's March Madness tournament, I scraped roster
data from &lt;a href="http://www.sports-reference.com/cbb/"&gt;Sports Reference&lt;/a&gt;. Then using the ggmaps R package, I geocoded
the hometowns of all of the players and plopped them on a map.&lt;/p&gt;
&lt;p&gt;So... what's the deal? Here's a map with all of the players from the US that were in the tournament, shaded by their position:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/All-Players-Map.png" alt="All-Players-Map.png" width="100%", height="100%"&gt;&lt;/p&gt;
&lt;p&gt;Lots of players from the urban areas across the US, not very surprising. Maybe a slight concentration of guards in the Northeast
running through NYC, Philly, and DC -- but overall nothing jumped out at me as being surprising. But this is a map of all of the playersin the 
tournament, not just the good teams. Maybe better players, defined as players from the top seeded teams, were from particular areas of the country?&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/seed-map.png" alt="Seeded-NCAA-Map" width="100%", height="100%"&gt;&lt;/p&gt;
&lt;p&gt;Looks like a group of solid players, or at least players on solid teams, are from the midwest. Wisconsin, Indiana, Illinois, Michigan, don't have very high
population density but have quite a few players in the tournament this year. The majority of them seem to be from 8 seeds or better.&lt;/p&gt;
&lt;p&gt;Lastly, I started wondering about individual teams and how widely spread their player's hometowns were. You always hear about the kid that's from the school's 
backyard, grew up a fan, and now is leading them to the Final Four. To get at what I'll call the "Team Spread" metric, I calculated the distance between each possible
pair of a team's American-born players and took the average. This gives the average distance players grew up from each other.&lt;/p&gt;
&lt;p&gt;Here are the 68 teams by their average distance between player hometowns....&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;College&lt;/th&gt;
&lt;th&gt;Team Spread (Miles)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;northern-iowa&lt;/td&gt;
&lt;td&gt;156&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;villanova&lt;/td&gt;
&lt;td&gt;173&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;purdue&lt;/td&gt;
&lt;td&gt;235&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;xavier&lt;/td&gt;
&lt;td&gt;353&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;middle-tennessee&lt;/td&gt;
&lt;td&gt;363&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;green-bay&lt;/td&gt;
&lt;td&gt;363&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;chattanooga&lt;/td&gt;
&lt;td&gt;365&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;west-virginia&lt;/td&gt;
&lt;td&gt;397&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dayton&lt;/td&gt;
&lt;td&gt;401&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;indiana&lt;/td&gt;
&lt;td&gt;426&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fairleigh-dickinson&lt;/td&gt;
&lt;td&gt;430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;north-carolina&lt;/td&gt;
&lt;td&gt;454&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;syracuse&lt;/td&gt;
&lt;td&gt;458&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;holy-cross&lt;/td&gt;
&lt;td&gt;522&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;butler&lt;/td&gt;
&lt;td&gt;560&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;michigan-state&lt;/td&gt;
&lt;td&gt;573&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pittsburgh&lt;/td&gt;
&lt;td&gt;598&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cincinnati&lt;/td&gt;
&lt;td&gt;640&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;texas-tech&lt;/td&gt;
&lt;td&gt;655&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;south-dakota-state&lt;/td&gt;
&lt;td&gt;667&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;iowa-state&lt;/td&gt;
&lt;td&gt;691&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;southern&lt;/td&gt;
&lt;td&gt;712&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;oklahoma&lt;/td&gt;
&lt;td&gt;715&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;texas&lt;/td&gt;
&lt;td&gt;727&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fresno-state&lt;/td&gt;
&lt;td&gt;762&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vanderbilt&lt;/td&gt;
&lt;td&gt;763&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;iowa&lt;/td&gt;
&lt;td&gt;765&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hampton&lt;/td&gt;
&lt;td&gt;775&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;texas-am&lt;/td&gt;
&lt;td&gt;793&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;north-carolina-asheville&lt;/td&gt;
&lt;td&gt;844&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wisconsin&lt;/td&gt;
&lt;td&gt;894&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;north-carolina-wilmington&lt;/td&gt;
&lt;td&gt;1,006&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;arkansas-little-rock&lt;/td&gt;
&lt;td&gt;1,112&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;yale&lt;/td&gt;
&lt;td&gt;1,171&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;virginia-commonwealth&lt;/td&gt;
&lt;td&gt;1,261&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;iona&lt;/td&gt;
&lt;td&gt;1,285&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;notre-dame&lt;/td&gt;
&lt;td&gt;1,319&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;florida-gulf-coast&lt;/td&gt;
&lt;td&gt;1,337&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;providence&lt;/td&gt;
&lt;td&gt;1,340&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;buffalo&lt;/td&gt;
&lt;td&gt;1,350&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;stephen-f-austin&lt;/td&gt;
&lt;td&gt;1,368&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;connecticut&lt;/td&gt;
&lt;td&gt;1,387&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;baylor&lt;/td&gt;
&lt;td&gt;1,416&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kansas&lt;/td&gt;
&lt;td&gt;1,449&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;austin-peay&lt;/td&gt;
&lt;td&gt;1,467&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;weber-state&lt;/td&gt;
&lt;td&gt;1,510&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;temple&lt;/td&gt;
&lt;td&gt;1,513&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;stony-brook&lt;/td&gt;
&lt;td&gt;1,518&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tulsa&lt;/td&gt;
&lt;td&gt;1,521&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;michigan&lt;/td&gt;
&lt;td&gt;1,523&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;miami-fl&lt;/td&gt;
&lt;td&gt;1,533&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;arizona&lt;/td&gt;
&lt;td&gt;1,551&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;maryland&lt;/td&gt;
&lt;td&gt;1,567&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;saint-josephs&lt;/td&gt;
&lt;td&gt;1,594&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;colorado&lt;/td&gt;
&lt;td&gt;1,802&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cal-state-bakersfield&lt;/td&gt;
&lt;td&gt;1,830&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wichita-state&lt;/td&gt;
&lt;td&gt;1,836&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;seton-hall&lt;/td&gt;
&lt;td&gt;1,838&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;duke&lt;/td&gt;
&lt;td&gt;1,908&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;utah&lt;/td&gt;
&lt;td&gt;1,921&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;oregon&lt;/td&gt;
&lt;td&gt;1,958&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;gonzaga&lt;/td&gt;
&lt;td&gt;2,067&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;virginia&lt;/td&gt;
&lt;td&gt;2,174&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kentucky&lt;/td&gt;
&lt;td&gt;2,273&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;california&lt;/td&gt;
&lt;td&gt;2,279&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;southern-california&lt;/td&gt;
&lt;td&gt;2,426&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;oregon-state&lt;/td&gt;
&lt;td&gt;2,752&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hawaii&lt;/td&gt;
&lt;td&gt;3,704&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Northern Iowa! Basically each of these kids slept in the bunkbeds above another teammate... But some top teams are right there at the top of the Team Spread list. All of the 
Final Four teams have a Team Spread of less than 1,000 miles. Villanova is right there in second with a whopping 173 miles. All West Coast teams at the other end of the spectrum, with 
Hawaii coming logically in last. But pretty nuts they have, on average, an additional 1,000 miles between players' hometowns.  &lt;/p&gt;
&lt;p&gt;One last set of maps for good measure -- take a look at where the kids are from on the two teams in the finals: 'Nova and UNC (can you tell which is which? ha). If closeness is any indicator of success, I better hurry and 
get some money on the boys from Philly.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/Villanova.png" alt="NCAA-Villanova" width="100%", height="100%"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/North-Carolina.png" alt="NCAA-UNC" width="100%", height="100%"&gt;&lt;/p&gt;</content><category term="R"></category><category term="Data viz"></category></entry><entry><title>Text Analysis of Craigslist Missed Connections</title><link href="http://bheal521.github.io/posts/2014/Craigslist-Missed-Connections/" rel="alternate"></link><published>2014-10-15T00:00:00-04:00</published><updated>2014-10-15T00:00:00-04:00</updated><author><name>Ben Healy</name></author><id>tag:bheal521.github.io,2014-10-15:/posts/2014/Craigslist-Missed-Connections/</id><summary type="html">&lt;p&gt;Sentiment and Cluster Analysis&lt;/p&gt;</summary><content type="html">&lt;p&gt;For a recent text analytics project, I took a look in a deep, dark corner of the Internet... at the one and only &lt;a href="https://newyork.craigslist.org/search/mis"&gt;Craigslist Missed Connections&lt;/a&gt;. 
For those unfamiliar with such shenanigans, Missed Connections are a sort of social platform for folks that had some sort of encounter with a stranger that they were interested in, but
never exchanged contact information with. I took a stab at estimating the sentiment of posts and then attempted to cluster them to try and get a sense of some of the main themes 
within the posts. Below is an example post, with all of the available information associated with a Missed Connection:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Bank Official - m4w - (Reston)&lt;/p&gt;
&lt;p&gt;Just wanted to say you're stunningly beautiful :) I was at your banking center in Reston. I was in a counter when you walked in and we 
looked at each other and said Hi. Wish there was a chance for a longer conversation! (Oct 15th morning and it was raining) &lt;/p&gt;
&lt;p&gt;9:51pm October 16, 2014&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the above example we have the title of the post (Bank Official), the type of post (m4w), the location (Reston), the body of the post, and the date/time that the post was submitted.
The title and body of the post are just raw text, but the post type (m4w) stands for man-for-woman, meaning a man submitted the post and the person he is interested in trying 
to find is a woman. In addition to m4w, other post types include m4m, w4m, w4w, etc. I chose to focus my little experiment on the East coast and, in particular 
on the following six cities: Boston, New York City, Baltimore, Washington DC, Raleigh, and Charlotte. In total I looked at about 4,500 posts. These posts ranged in length from just a handful
of words, to what seemed like poorly written novellas. Below shows some summary info on the number of posts we collected, the types of posts, gender of posters, and when they posted.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/CList-Post-Summary.png" alt="CList-Post-Summary" width="1000", height="500"&gt;&lt;/p&gt;
&lt;h2&gt;PART I: Sentiment Analysis&lt;/h2&gt;
&lt;p&gt;Using the ANEW Python package I was able to estimate the sentiment of the missed connections on a 2 different scales: arousal and valence. While use of the term 'arousal' might lead you
to believe that I looked at how sexually explicit these texts were, arousal in this sense is focused on the level of excitement within a post. The valence score was used to show
how pleased a body of text was. So, for example if a post had a high valence score and a high arousal score, this could be translated to an emotion such as ecstatic. On the other side
of the scales, low valence and low arousal could be more in the ballpark of depressed.&lt;/p&gt;
&lt;p&gt;Below you can check out the scores of the posts with some options to filter some of the data. Blue dots are posts written by men, green are written by women. The size of the posts is
the "total variance" of the post. The way the ANEW package scores sentiment, is to individually score words within a post. There are obvious drawbacks to this, most obviously, the fact that
a lot of context is lost when looking only at individual words... but it's better than nothing. So the "total variance" of a post is just the sum of the variance in word-scores on the arousal scales
and the variance of the word scores for valence scale. It was an attempt to find posts that might have been average in valence and/or arousal, but had high variability, indicating they contained
a couple of words that were very extreme. Unfortunately, it became clear quickly that the sentiment of these posts is just very similar. I tried slicing by location of the post, 
gender of the poster, gender of the person being posted about, time of day the post was written, etc... and I couldn't see significant differences in the sentiment of these posts. And
there went my theory that late night posts would be sad and depressing while daytime posts would be more optimistic and happy. Such is life.&lt;/p&gt;
&lt;p&gt;Below the sentiment scores of posts are the most frequently used words within posts (relative frequency indicated by the size of the bubble), with their associated sentiment scores. Hopefully 
this gives you an idea of what words are in the ANEW dictionary that were scored to create the overall sentiment scores for posts.&lt;/p&gt;
&lt;script type='text/javascript' src='https://public.tableausoftware.com/javascripts/api/viz_v1.js'&gt;&lt;/script&gt;

&lt;div class='tableauPlaceholder' style='width: 924px; height: 629px;'&gt;&lt;noscript&gt;&lt;a href='#'&gt;&lt;img alt='Clusters ' src='https:&amp;#47;&amp;#47;public.tableausoftware.com&amp;#47;static&amp;#47;images&amp;#47;Cr&amp;#47;CraigslistMissedConnections&amp;#47;Clusters&amp;#47;1_rss.png' style='border: none' /&gt;&lt;/a&gt;&lt;/noscript&gt;&lt;object class='tableauViz' width='924' height='629' style='display:none;'&gt;&lt;param name='host_url' value='https%3A%2F%2Fpublic.tableausoftware.com%2F' /&gt; &lt;param name='site_root' value='' /&gt;&lt;param name='name' value='CraigslistMissedConnections&amp;#47;Clusters' /&gt;&lt;param name='tabs' value='no' /&gt;&lt;param name='toolbar' value='yes' /&gt;&lt;param name='static_image' value='https:&amp;#47;&amp;#47;public.tableausoftware.com&amp;#47;static&amp;#47;images&amp;#47;Cr&amp;#47;CraigslistMissedConnections&amp;#47;Clusters&amp;#47;1.png' /&gt; &lt;param name='animate_transition' value='yes' /&gt;&lt;param name='display_static_image' value='yes' /&gt;&lt;param name='display_spinner' value='yes' /&gt;&lt;param name='display_overlay' value='yes' /&gt;&lt;param name='display_count' value='yes' /&gt;&lt;/object&gt;&lt;/div&gt;

&lt;div style='width:924px;height:22px;padding:0px 10px 0px 0px;color:black;font:normal 8pt verdana,helvetica,arial,sans-serif;'&gt;&lt;div style='float:right; padding-right:8px;'&gt;&lt;a href='http://www.tableausoftware.com/public/about-tableau-products?ref=https://public.tableausoftware.com/views/CraigslistMissedConnections/Clusters' target='_blank'&gt;Learn About Tableau&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2&gt;PART 2: Cluster Analysis&lt;/h2&gt;
&lt;p&gt;While sentiment analysis turned out to be mostly futile, I did experience some success in clustering posts. Using SAS Enterprise Miner, I was able to come up with the 
following six clusters:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;Cluster&lt;/th&gt;
&lt;th align="left"&gt;Descriptive Terms&lt;/th&gt;
&lt;th align="left"&gt;% of Posts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Cluster #1&lt;/td&gt;
&lt;td align="left"&gt;Shirt, short, smile, train, wear, black, blue, brown, hair, white, color, beautiful&lt;/td&gt;
&lt;td align="left"&gt;20%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Cluster #2&lt;/td&gt;
&lt;td align="left"&gt;Parking lot, car, drive, park, driving, gas, kind, color, turn, leave, white&lt;/td&gt;
&lt;td align="left"&gt;5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Cluster #3&lt;/td&gt;
&lt;td align="left"&gt;Day, feel, good, happy, heart, life, love, miss, thing, want, always, year&lt;/td&gt;
&lt;td align="left"&gt;20%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Cluster #4&lt;/td&gt;
&lt;td align="left"&gt;Chat, hear, hope, meet, number, shot, talk, friend, long, night, know, chance&lt;/td&gt;
&lt;td align="left"&gt;30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Cluster #5&lt;/td&gt;
&lt;td align="left"&gt;Interest, nice, guy, afternoon, check, hit, hot, white, today, look, great&lt;/td&gt;
&lt;td align="left"&gt;9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Cluster #6&lt;/td&gt;
&lt;td align="left"&gt;Make eye, eye, hit, hot, time, contact few, couple, man, sexy, today, work&lt;/td&gt;
&lt;td align="left"&gt;16%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The three clusters with the largest percentage of the posts within my sample set are clusters 1, 3 and 4. I think these are pretty good representations of some of the topics that In
saw anecdotally when manually looking through some of these posts. The first cluster is about physical description -- the colors appearing in the important terms are likely hair or eye color,
words like "smile", "beautiful", and "wear" speak for themselves. The third cluster is clearly an emotionally focused topic. The words are largely centered on the emotions felt by the poster,
presumably when they encountered this stranger they now are hoping to connect with. Finally, the fourth cluster appears to be focused on the communication aspect of these posts. Many of these
posts deal with situations where individuals had a chance to talk to each other, but failed to get contact information before parting ways. Words like "chat", "talk", "chance", are indicators
that the posts are describing these encounters where communication did occur.&lt;/p&gt;
&lt;p&gt;Clusters are calculated using &lt;a href="http://en.wikipedia.org/wiki/Singular_value_decomposition"&gt;singular value decomposition&lt;/a&gt;, a process that essentially looks at terms and even combinations of words in order to virtually plot each post on a multi-dimensional
set of axes that, when combined, represent all of the words found across all of the posts in the data set. The key, is that these axes are created in order, in order of the amount of variability
they help to explain within the posts. While the topic itself is difficult to explain, the important aspect is that these created axes with which posts are now plotted on are created in order
by the amount of variability they explain. In other words, the first axis is the best at separating out posts by type. Below, I've taken the first three axes that result from singular value decomposition
and plotted the posts that fall within the clusters 1, 3, and 4. You can see how they are really separated! &lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/Clist-Missed_Connect_SVD.png" alt="Clist-Missed_Connect_SVD" width="100%", height="100%"&gt;&lt;/p&gt;</content><category term="Tableau"></category><category term="Data viz"></category></entry><entry><title>Tableau</title><link href="http://bheal521.github.io/posts/2014/Tableau-Trials/" rel="alternate"></link><published>2014-09-01T00:00:00-04:00</published><updated>2014-09-01T00:00:00-04:00</updated><author><name>Ben Healy</name></author><id>tag:bheal521.github.io,2014-09-01:/posts/2014/Tableau-Trials/</id><summary type="html">&lt;p&gt;Trials and Tribulations in Tableau&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's been a while since I've put something up here... sorry to the 3 people that have ever visited the site for the wait! 
Several months have flown by largely due to the fact that I am back at school! One of the new toys I've had a chance to play 
with since being back in the classroom is Tableau, a well known data-visualization tool that is pretty powerful while still being 
very accessible to less-technical folks. There is no programming necessary to make it go, just point and click.&lt;/p&gt;
&lt;p&gt;Currently I only have access to Tableau Public, a free-for-students version of the software that only allows you to save your work to the web, 
nothing can be saved locally. But with that limitation, you get access to a whole host of different public data including geographic and demographic
data from the US Census. Additionally, with Tableau you can easily add some user-interaction to your visualizations and, with the public version, these
interactions are carried over to the web. So without having to tinker around with any JavaScript libraries, boom, you've got a fun web tool.&lt;/p&gt;
&lt;p&gt;Using some of the freely available data in my version of Tableau, I created the visualization below that takes a look at the adult obesity rates in the United
States. Selecting a state on the right hand panel should zoom the graphic into that state and give you some more specific, corresponding information for that state in the 
scatter plots below the map. Let me know what you think below, have you ever used Tableau public? Anything cool I should know?&lt;/p&gt;
&lt;script type='text/javascript' src='https://public.tableausoftware.com/javascripts/api/viz_v1.js'&gt;&lt;/script&gt;

&lt;div class='tableauPlaceholder' style='width: 804px; height: 669px;'&gt;&lt;noscript&gt;&lt;a href='#'&gt;&lt;img alt='Dashboard 1 ' src='https:&amp;#47;&amp;#47;public.tableausoftware.com&amp;#47;static&amp;#47;images&amp;#47;Ad&amp;#47;AdultObesityintheUnitedStates&amp;#47;Dashboard1&amp;#47;1_rss.png' style='border: none' /&gt;&lt;/a&gt;&lt;/noscript&gt;&lt;object class='tableauViz' width='804' height='669' style='display:none;'&gt;&lt;param name='host_url' value='https%3A%2F%2Fpublic.tableausoftware.com%2F' /&gt; &lt;param name='site_root' value='' /&gt;&lt;param name='name' value='AdultObesityintheUnitedStates&amp;#47;Dashboard1' /&gt;&lt;param name='tabs' value='no' /&gt;&lt;param name='toolbar' value='yes' /&gt;&lt;param name='static_image' value='https:&amp;#47;&amp;#47;public.tableausoftware.com&amp;#47;static&amp;#47;images&amp;#47;Ad&amp;#47;AdultObesityintheUnitedStates&amp;#47;Dashboard1&amp;#47;1.png' /&gt; &lt;param name='animate_transition' value='yes' /&gt;&lt;param name='display_static_image' value='yes' /&gt;&lt;param name='display_spinner' value='yes' /&gt;&lt;param name='display_overlay' value='yes' /&gt;&lt;param name='display_count' value='yes' /&gt;&lt;/object&gt;&lt;/div&gt;

&lt;div style='width:804px;height:22px;padding:0px 10px 0px 0px;color:black;font:normal 8pt verdana,helvetica,arial,sans-serif;'&gt;&lt;div style='float:right; padding-right:8px;'&gt;&lt;a href='http://www.tableausoftware.com/public/about-tableau-products?ref=https://public.tableausoftware.com/views/AdultObesityintheUnitedStates/Dashboard1' target='_blank'&gt;Learn About Tableau&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;</content><category term="Tableau"></category><category term="Data viz"></category></entry><entry><title>World Cup</title><link href="http://bheal521.github.io/posts/2014/History-of-the-World-Cup/" rel="alternate"></link><published>2014-06-03T00:00:00-04:00</published><updated>2014-06-03T00:00:00-04:00</updated><author><name>Ben Healy</name></author><id>tag:bheal521.github.io,2014-06-03:/posts/2014/History-of-the-World-Cup/</id><summary type="html">&lt;p&gt;Who has made it to the world cup? Who has won?&lt;/p&gt;</summary><content type="html">&lt;p&gt;With the World Cup almost a week away, I've been reading up on the teams headed to Brazil and trying to stay positive 
about the chances that the US have of making it out of the group stage. While there is no shortage of coverage on all of the
major teams and players, I enjoyed Professor Stephen Hawking's &lt;a href="http://blog.paddypower.com/wp-content/uploads/2014/05/hawking-report_WC2014.pdf"&gt;World Cup Study&lt;/a&gt;
that looks at the conditions that best suit England in a World Cup match. What it lacks in quantitative rigor, it more than makes up for in
playfulness. Hawking looks at a number of factors and their correlation to England's success on the pitch, including: distance from home, game-day temperature,
time of kick-off, color of jersey worn, nationality of the ref, and the team's formation. According to him, England plan to employ a 4-3-3 formation and are
crossing their fingers for a European ref and a 3pm kick-off time.&lt;/p&gt;
&lt;p&gt;As I've prepared myself to watch endless hours of this World Cup, I started wondering about past cups. Analysts keep mentioning that no European team has
won a World Cup on South American soil. But I don't have a very good idea of who has won in general. I remember watching the past couple... going backwards I could
remember that Spain, Italy and Brazil had won the past three. But I struggled to remember that France had won in '98. Though I think I might still own a tee shirt
with the logo from the 98' World Cup on it... a weird bird of sorts...&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/France98mascot.png" alt="France98-mascot" width="50%", height="50%"&gt;&lt;/p&gt;
&lt;p&gt;I headed to Wikipedia to scrape the location of each World Cup, the teams that attended, and their respective performances. Once I had this information,
I threw together a map of each World Cup showing all countries that attended in blue along with the host country in red. Using &lt;a href="http://http://gifmaker.me/"&gt;GifMaker.me&lt;/a&gt;
I stitched the images together to create a single GIF of the history of the World Cup.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/WC_Years.gif" alt="WC-history" width="100%", height="100%"&gt;&lt;/p&gt;
&lt;p&gt;In more succinct form, the map below shows all countries that have attended a World Cup and is shaded to indicate the number of World Cups they have been to.
Brazil, Germany, and Italy are in a league of their own in terms of World Cup pedigree -- each has been to more than fifteen cups (there have only been 19!).
The most striking thing I took away from the map below was just how minimally the African continent has broken into World Cups. The only country that has 
consistently qualified is Cameroon, with 7 appearances.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/Worldcup_Appearances_Withlegend.png" alt="WC-appearances" width="100%", height="100%"&gt;&lt;/p&gt;
&lt;p&gt;Making it to the World Cup is absolutely an achievement, but those that have won belong to a truly elite group. There are only 8 countries that have won, 
they are listed in the table below and shown in the subsequent map.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;Country&lt;/th&gt;
&lt;th align="left"&gt;# World Cup Wins&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Brazil&lt;/td&gt;
&lt;td align="left"&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Italy&lt;/td&gt;
&lt;td align="left"&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Germany&lt;/td&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Argentina&lt;/td&gt;
&lt;td align="left"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Uruguay&lt;/td&gt;
&lt;td align="left"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Spain&lt;/td&gt;
&lt;td align="left"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;France&lt;/td&gt;
&lt;td align="left"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;England&lt;/td&gt;
&lt;td align="left"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/WorldCup_Winners.png" alt="WC-appearances" width="100%", height="100%"&gt;&lt;/p&gt;</content><category term="R"></category><category term="soccer"></category><category term="world cup"></category></entry><entry><title>Continued Analysis of Pitchfork Reviews</title><link href="http://bheal521.github.io/posts/2014/Continued-Analysis-of-Pitchfork-Reviews/" rel="alternate"></link><published>2014-04-06T00:00:00-04:00</published><updated>2014-04-06T00:00:00-04:00</updated><author><name>Ben Healy</name></author><id>tag:bheal521.github.io,2014-04-06:/posts/2014/Continued-Analysis-of-Pitchfork-Reviews/</id><summary type="html">&lt;p&gt;Does Pitchfork penalize bands that aren't new?&lt;/p&gt;</summary><content type="html">&lt;p&gt;The last few days I've looked over a lot of great information on natural language processing and sentiment analysis. There are plenty of really great examples 
of how this has been done. Many of the &lt;a href="http://www.sjwhitworth.com/sentiment-analysis-in-python-using-nltk/"&gt;examples&lt;/a&gt; I found dealt with classifying tweets 
as negative or positive. But this same classification of text and machine learning is how something like spam-email works. Ultimately, what I'd like to do is build 
a model that learns to predict the score given to a Pitchfork review by analyzing its text. I've started working on it, but think that it's going to take some 
serious effort to figure out how to best train the model on the ~10,000 reviews I have in my database in order to look at new reviews and correctly classify 
the score.&lt;/p&gt;
&lt;p&gt;But before really diving into some more serious natural language processing and machine learning, I thought I'd look for an answer to a question I've had 
about Pitchfork for a while. That is, if a band that has been reviewed by Pitchfork before comes out with additional albums, are those albums penalized in score
for not being new enough? For example, there is a band called &lt;a href="http://pitchfork.com/artists/28390-sleigh-bells/"&gt;Sleigh Bells&lt;/a&gt; that have had three of their
albums reviewed by Pitchfork. Their first album, &lt;a href="http://pitchfork.com/reviews/albums/14251-treats/"&gt;Treats&lt;/a&gt;, was released in 2010 and brought this new 
&lt;em&gt;BIG&lt;/em&gt; sound that was described by the reviewer as music that,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;just seemed bigger than it had before, 
like it took up more space and hit&lt;br&gt;
with more force and went further than 
once seemed possible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The writer reviewed the album very favorably and awarded it an 8.7, enough to get it on the &lt;a href="http://pitchfork.com/reviews/best/albums/"&gt;Best New Music&lt;/a&gt; portion of the website, which I'm sure
 had lots of benefits in terms of gaining new listeners. I know that is where I first saw them. But Sleigh Bells released two more albums in the three years that
 followed and received an 8.2 and then a 5.9. Now, just because a band created a great album does not mean that all albums after it will also be great. I realize
 this and also understand that the reviewing of music is about as subjective as it gets... But I believe that a band that creates a first album worthy of a 
 high Pitchfork rating is more likely to do so again in future work. In this specific example, I don't even disagree on the order in which
 the albums are ranked. As a Sleigh Bells fan, I think they got progressively worse. But I have a tough time rationalizing a drop from 8.2 on their 
 &lt;a href="http://pitchfork.com/reviews/albums/16297-reign-of-terror/"&gt;second album&lt;/a&gt; to a 5.9 on their 
 &lt;a href="http://pitchfork.com/reviews/albums/18594-sleigh-bells-bitter-rivals/"&gt;third&lt;/a&gt;. The big change over this three year period was Sleigh Bell's steady 
 climb into main steam music. My hypothesis was that Pitchfork penalizes bands with abnormally low scores after their initial releases due to the fact those
 bands become known and &lt;em&gt;so0o0o mainstream bro&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To test my hypothesis, I first looked at the bands that had been reviewed the most on Pitchfork. Why, you ask? Well -- it doesn't really provide any useful
information in answering my question but I was sorta wondering... and it's my blog post so I'll do what I want! The table below has the bands that have
had at least 10 albums reviewed by Pitchfork along with their average score and average word length of the reviews written on them.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;Artist&lt;/th&gt;
&lt;th align="left"&gt;# Albums Reviewed&lt;/th&gt;
&lt;th align="left"&gt;Average Score&lt;/th&gt;
&lt;th align="left"&gt;Average Review Word Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Animal Collective&lt;/td&gt;
&lt;td align="left"&gt;10&lt;/td&gt;
&lt;td align="left"&gt;7.9&lt;/td&gt;
&lt;td align="left"&gt;769&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Guided By Voices&lt;/td&gt;
&lt;td align="left"&gt;10&lt;/td&gt;
&lt;td align="left"&gt;6.8&lt;/td&gt;
&lt;td align="left"&gt;836&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Lil Wayne&lt;/td&gt;
&lt;td align="left"&gt;10&lt;/td&gt;
&lt;td align="left"&gt;6.6&lt;/td&gt;
&lt;td align="left"&gt;807&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Mogwai&lt;/td&gt;
&lt;td align="left"&gt;10&lt;/td&gt;
&lt;td align="left"&gt;6.9&lt;/td&gt;
&lt;td align="left"&gt;670&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;R.E.M.&lt;/td&gt;
&lt;td align="left"&gt;11&lt;/td&gt;
&lt;td align="left"&gt;8.2&lt;/td&gt;
&lt;td align="left"&gt;883&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Xiu Xiu&lt;/td&gt;
&lt;td align="left"&gt;11&lt;/td&gt;
&lt;td align="left"&gt;7.1&lt;/td&gt;
&lt;td align="left"&gt;709&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Four Tet&lt;/td&gt;
&lt;td align="left"&gt;12&lt;/td&gt;
&lt;td align="left"&gt;7.5&lt;/td&gt;
&lt;td align="left"&gt;748&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Robert Pollard&lt;/td&gt;
&lt;td align="left"&gt;15&lt;/td&gt;
&lt;td align="left"&gt;5.9&lt;/td&gt;
&lt;td align="left"&gt;615&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;The Beatles&lt;/td&gt;
&lt;td align="left"&gt;19&lt;/td&gt;
&lt;td align="left"&gt;9.1&lt;/td&gt;
&lt;td align="left"&gt;978&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Xiu Xiu was definitely a surprise for me, I didn't realize they had so much music out... Also, remember that this is just the most recent 10,000 music reviews
on the site so it is entirely possible that some of these bands have additional albums that were reviewed very early on in Pitchfork's history that I did not
scrape. It is clear that R.E.M. is given lots of respect, but nobody touches The Beatles, cuz duh.&lt;/p&gt;
&lt;p&gt;But back to my question. In order to see whether or not bands got progressively worse scores the more albums they came out with, I plotted albums by their
release number (a band's n&lt;em&gt;th&lt;/em&gt; album) and the score it received. As is expected, the number of data points drops off as you get to higher numbered album releases.
This is because, by definition, every band that has a review on Pitchfork has a first album, but fewer have a second album. Even fewer have a third album that was
reviewed, and so on. By the time we get out past ten, the only bands for which there are data points are the ones included in the table above. With that in mind,
take a look:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/pitchfork_bias.png" alt="Pitchfork-bias" width="100%", height="110%"&gt;&lt;/p&gt;
&lt;p&gt;The dots plotted in black represent all of the album reviews, and the larger blue dots represent the average score given to that album number. For example,
the average score given to a band's 9th album reviewed on Pitchfork was just shy of 8. While it's hard to be confident in any findings to the right half of 
the graph, it's interesting to see that the average score is actually increasing slightly as we go from a band's first album, to their second, all the 
way through their 6th. Looks like I was wrong. &lt;/p&gt;
&lt;h2&gt;Does Pitchfork Give More Words to Higher Rated Bands?&lt;/h2&gt;
&lt;p&gt;While looking at the word count of the various writers at Pitchfork, I began to wonder if Pitchfork reviews were longer for bands they rated highly
and shorter for others. The graph below plots all of the bands reviewed by Pitchfork by their average review score versus the average word count for their
reviews. The line in blue running through the graph is a sample regression model to help visualize the relationship. &lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/pitchfork_scoreVSlength.png" alt="Pitchfork-score-vs-length" width="100%", height="110%"&gt;&lt;/p&gt;
&lt;p&gt;You can see that the bands that are rated the highest definitely seem to get some extra attention. What is most striking, is the cluster of bands hovering
right around a rating of 7 and a word count of about 600. It seems that Pitchfork likely has a suggested length for its reviews, and that the vast majority
of bands are getting scores near 7. This could be for a number of reasons, but I'd imagine that what might seem like a high average for scores is because only
albums and bands that are pre-screened to some extent get selected for a review. Pitchfork isn't just reviewing every album they can get their hands on.&lt;/p&gt;
&lt;p&gt;See something interesting here? Any questions or suggestions on further exploration of this data? All feedback is welcome below in the comments, or feel free to 
get in touch with me using the method of your preference on my &lt;a href="http://bheal521.github.io/pages/contact.html"&gt;contacts page&lt;/a&gt;.&lt;/p&gt;</content><category term="R"></category><category term="music"></category><category term="Pitchfork"></category></entry><entry><title>Preliminary Pitchfork Review Text Analysis</title><link href="http://bheal521.github.io/posts/2014/Preliminary-Pitchfork-Review-Text-Analysis/" rel="alternate"></link><published>2014-04-03T00:00:00-04:00</published><updated>2014-04-03T00:00:00-04:00</updated><author><name>Ben Healy</name></author><id>tag:bheal521.github.io,2014-04-03:/posts/2014/Preliminary-Pitchfork-Review-Text-Analysis/</id><summary type="html">&lt;p&gt;Scraping Pitchfork music reviews and taking a closer look at the reviewers that shape my musical preferences.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Everyone likes to think that they listen to the &lt;strong&gt;best&lt;/strong&gt; music. If you're a bit of a music snob, you not only think that your music taste is the best, but that it's unique and eclectic -- not something
that someone who listens to only the Top 40 would understand. I absolutely fall under this category; I can sometimes even convince myself that my general preferences aren't curated for me by 
&lt;a href="http://pitchfork.com"&gt;Pitchfork&lt;/a&gt;. But in reality, the reviews, ratings and general opinion of this site have had a huge influence on my musical preferences and, in using the site
as my musical gospel, I ensure that my "unique" taste in music is shared by the masses (just slightly smaller masses than those that listen only to radio hits). So, with all of my angsty appreciation 
of this site in mind, I thought it would be interesting to take a closer look at the reviews and reviewers that have helped shape my &lt;em&gt;melodic proclivities&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Web Scraping&lt;/h2&gt;
&lt;p&gt;Using Python and a package called &lt;a href="http://www.crummy.com/software/BeautifulSoup/"&gt;BeautifulSoup&lt;/a&gt;, I created a program that scraped the most recent 10,000 reviews published on Pitchfork. 
I created a &lt;a href="http://www.mysql.com/"&gt;MySQL&lt;/a&gt; database that stored the text of each review along with some other general information (band, album, record label, reviewer, date...). I then used R
to query the database for analysis. &lt;/p&gt;
&lt;p&gt;The following Python code shows how BeautifulSoup reads in a web page, turns it into &lt;em&gt;soup&lt;/em&gt;, and then searches through the HTML tags until it gets to the node of interest. I've found using the Chrome
browser's developer tools is an awesome way to sift through a site's source HTML to figure out where the information you're interested in scraping lives.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#this package is also needed to read in the website and grab the HTML&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;urllib2&lt;/span&gt; 
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bs4&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;
&lt;span class="n"&gt;baseurl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://pitchfork.com/reviews/albums/19075-cloud-nothings-here-and-nowhere-else/&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;baseurl&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;## Extract the meta-data for the album&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ul&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findAll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ul&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;class&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;review-meta&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;div&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ul&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findAll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;div&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;class&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;info&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}):&lt;/span&gt;
        &lt;span class="n"&gt;artist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;div&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;album&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;div&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;div&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findAll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;span&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;div&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;rev_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;div&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h4&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;reviewer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;div&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h4&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;

&lt;span class="c1"&gt;#Extract the body of the review&lt;/span&gt;
&lt;span class="n"&gt;editorial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;div&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;class&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;editorial&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ascii&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ignore&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For those interested in web scraping, I'd suggest Python and BeautifulSoup -- I've been using it for a little while now and have found it to be very straightforward. But there are other, well-loved
packages around, most notably &lt;a href="http://scrapy.org/"&gt;Scrapy&lt;/a&gt;. Coincidentally, there is a more point-and-click-friendly approach built on Scrapy that was just released. It's called &lt;a href="http://blog.scrapinghub.com/2014/04/01/announcing-portia/"&gt;Portia &lt;/a&gt;,
and it is still in development but you can check them out on GitHub. It appears to be great for people with limited coding skills and for simple, well-structured sites.&lt;/p&gt;
&lt;h2&gt;Initial Results&lt;/h2&gt;
&lt;p&gt;So, what did I find?&lt;/p&gt;
&lt;p&gt;There were about 170 different writers that published at least one of the most recent 10,000 reviews on Pitchfork. But many had written one, or just a couple of reviews. Twenty-five writers
had published just one review and sixty-six writers had published ten or fewer reviews. The table below shows the general breakdown of how many writers wrote how many reviews:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;# of Reviews&lt;/th&gt;
&lt;th align="left"&gt;# of Writers&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;1&lt;/td&gt;
&lt;td align="left"&gt;25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;2-5&lt;/td&gt;
&lt;td align="left"&gt;27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;6-10&lt;/td&gt;
&lt;td align="left"&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;11-25&lt;/td&gt;
&lt;td align="left"&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;26-50&lt;/td&gt;
&lt;td align="left"&gt;26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;51-100&lt;/td&gt;
&lt;td align="left"&gt;19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;101-250&lt;/td&gt;
&lt;td align="left"&gt;22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;250-500&lt;/td&gt;
&lt;td align="left"&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;500+&lt;/td&gt;
&lt;td align="left"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Who is that lone individual that has written more than FIVE HUNDRED reviews, you ask? Well, if you are familiar with Pitchfork it will come as no surprise that it is the one and only &lt;a href="http://pitchfork.com/staff/ian-cohen/"&gt;Ian Cohen&lt;/a&gt;. 
I was most interested in the heavy-hitters, those that had written over 100 reviews -- so I subset my results to just them, of which there were 30. Below is a quick summary of those writers and their
stats. The bars are shaded according to the number of reviews the writer completed,  and the number floating above each bar is the average word count for that particular writer's reviews.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/Reviewers_BarChart.png" alt="Pitchfork-writers-stats" width="100%", height="110%"&gt;&lt;/p&gt;
&lt;p&gt;It's interesting to see that most of these writers have an average score somewhere right between 6 and 7, with a handful of exceptions. But nobody has an average below 6, even Ian Cohen --  who is just shy of
having the lowest average score. He's averaging slightly higher than a 6.2 but Adam Moerder is down at a 6.1. Wouldn't want my upcoming LP on his desk... There also appears to be no relationship
between a writer's average score given to an album and their average review length. Nor is there any relationship between the number of reviews a writer has done and the length of those reviews. But for a surprisingly tight 
range around the average scores given by these writers, the length of their reviews varies pretty significantly. The shortest average review length is just over 450 words while the longest is
up over 850 words. Towards the lengthy side, we find Mr. Ian Cohen -- writing an average of over 750 words per review.&lt;/p&gt;
&lt;p&gt;Next I took a look at the actual content of these reviews. Using R's text mining and word cloud packages, I did some very minimal clean-up of the text and created a word cloud showing some of
the terms that were used most frequently. Again, this was done using only the reviews of these "heavy hitters" -- writers with more than 100 reviews under their belt. The relative size of the word
indicates its frequency in the reviews. There's some debate over whether or not word clouds are useful. The &lt;a href="http://www.niemanlab.org/2011/10/word-clouds-considered-harmful/"&gt;h83rs&lt;/a&gt; say that it's essentially 
impossible to draw insight from a jumbled bag of word frequencies and that words are only meaningful in context. To that I say -- yeah you're probably right... but I think they look kinda cool 
and in this particular example they at least show me some of the more common adjectives used by Pitchfork writers to describe music. &lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/Reviews_WrdCld.png" alt="Pitchfork-review-wordcloud" style="margin:auto; width:75%;display:block"&gt;&lt;/p&gt;
&lt;p&gt;So, draw from the above what you will (or don't you h83r). From here, I plan to do some more advanced text analysis looking beyond just word frequencies. For example, I might explore word 
correlation/association with other words as well as some word clustering to get a better sense of context. I'm reading up a bit on sentiment-analysis as well, so I may try and sift through
each writers reviews to score their sentiment and see how that relates to their scoring of an album. But that is likely to be tough... the sarcasm-meter is high in some of the reviews 
which increases the difficulty when training a model to deal with that appropriately. But stay tuned!&lt;/p&gt;
&lt;p&gt;Do you have any comments on what I've found so far? Ideas for how to take it further? Leave a comment below or get in touch with me in some form or another 
on my &lt;a href="http://bheal521.github.io/pages/contact.html"&gt;Contact page&lt;/a&gt;!&lt;/p&gt;
&lt;h2&gt;&lt;em&gt;EDIT 4-4-14&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;I saw earlier today that Ian Cohen got his panties in a bunch when some d00d from Village Voice critiqued his recent &lt;a href="http://pitchfork.com/reviews/albums/19075-cloud-nothings-here-and-nowhere-else/"&gt;Cloud Nothings review&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/bheal521/bheal521.github.io/master/images/cohen_tweet.png" alt="ian_cohen_tweet" style="margin:auto; width:25%;display:block"&gt;&lt;/p&gt;
&lt;p&gt;Here's a link to the holier-than-thou piece: &lt;a href="http://blogs.villagevoice.com/music/2014/04/music_writing_no_nos.php"&gt;Stop Using Clichés...&lt;/a&gt;
In the midst of his blowhard-ing Mr Village-Voice goes on a rant about the use of the word "sonic" in music reviews. I went back and took a look at the word cloud above and... low and behold, there 
it is sitting right above the &lt;strong&gt;L&lt;/strong&gt; in 'album' (album is in red near the center of the image). Remember, this wordcloud is from the reviews of the 30 writers who have written the most in the 
past couple years not just Ian Cohen, but still -- kinda funny.&lt;/p&gt;</content><category term="python"></category><category term="R"></category><category term="text analysis"></category></entry></feed>